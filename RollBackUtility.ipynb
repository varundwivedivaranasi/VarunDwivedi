{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0700db19-8a07-4068-be88-3a86222634d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee3c3fd-c3b1-42f4-bd79-3c3626e38c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_path = \"abfss://gold@rmpyru.dfs.core.windows.net/Audit\"\n",
    "target_path = \"abfss://gold@rmpyru.dfs.core.windows.net/zomato\"\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"batch_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"source_path\", StringType(), True),\n",
    "    StructField(\"record_count\", LongType(), True),\n",
    "    StructField(\"delta_table_version\", LongType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"rollback_flag\", StringType(), True)  # 'Y' or 'N'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8dedf4-71a9-4a53-84f3-09e88cdbe792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to safely get Delta version\n",
    "def get_delta_version(path):\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, path)\n",
    "        history_df = delta_table.history()\n",
    "        if history_df.count() > 0:\n",
    "            return history_df.head(1)[0]['version']\n",
    "        else:\n",
    "            print(\"Delta table exists but has no version history.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Delta table not found at {path}. Initializing...\")\n",
    "        return None\n",
    "\n",
    "# Check and initialize if needed\n",
    "current_version = get_delta_version(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95929ba-e6f7-4531-8cc8-93debd6e7afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rollback_batch(batch_id: str, rollback_version: int):\n",
    "    \"\"\"\n",
    "    Rolls back the target Delta table to the specified version for the given batch_id.\n",
    "    Logs the rollback event in the audit table.\n",
    "    \"\"\"\n",
    "    # Load audit table and get metadata for the batch\n",
    "    audit_df = spark.read.format(\"delta\").load(audit_path)\n",
    "    rollback_info = (\n",
    "        audit_df.filter(col(\"batch_id\") == batch_id)\n",
    "                .orderBy(col(\"timestamp\").desc())\n",
    "                .limit(1)\n",
    "                .collect()[0]\n",
    "    )\n",
    "\n",
    "    source_path = rollback_info[\"source_path\"]\n",
    "    record_count = rollback_info[\"record_count\"]\n",
    "\n",
    "    # Read the target table as of rollback_version\n",
    "    restored_df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .option(\"versionAsOf\", rollback_version)\n",
    "        .load(target_path)\n",
    "    )\n",
    "\n",
    "    # Overwrite the current table with restored version\n",
    "    restored_df.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n",
    "\n",
    "    # Get current version after overwrite\n",
    "    #current_version = get_delta_version(target_path)\n",
    "\n",
    "    # Prepare rollback metadata\n",
    "    timestamp = datetime.now()\n",
    "    updated_metadata_df = spark.createDataFrame([\n",
    "        (\n",
    "            batch_id,\n",
    "            timestamp,\n",
    "            source_path,\n",
    "            record_count,\n",
    "            rollback_version,\n",
    "            \"ROLLEDBACK\",\n",
    "            \"Y\"\n",
    "        )\n",
    "    ], schema=metadata_schema)\n",
    "\n",
    "    # Log rollback event\n",
    "    updated_metadata_df.write.format(\"delta\").mode(\"append\").save(audit_path)\n",
    "\n",
    "    print(f\"✅ Rollback completed for batch {batch_id} to version {rollback_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d8bfe3-bb48-4613-b361-7ed0adeca723",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_and_trigger_rollback():\n",
    "    \"\"\"\n",
    "    Checks if the latest FAILED batch has a newer delta_table_version than the last SUCCESS batch.\n",
    "    If true, triggers rollback using rollback_batch(batch_id, rollback_version).\n",
    "    \"\"\"\n",
    "    # Load audit table\n",
    "    audit_df = spark.read.format(\"delta\").load(audit_path)\n",
    "\n",
    "    # Get latest FAILED batch\n",
    "    failed_df = audit_df.filter(col(\"status\") == \"FAILED\").orderBy(col(\"timestamp\").desc())\n",
    "    failed_batch = failed_df.limit(1).collect()\n",
    "\n",
    "    # Get latest SUCCESS batch\n",
    "    success_df = audit_df.filter(col(\"status\") == \"SUCCESS\").orderBy(col(\"timestamp\").desc())\n",
    "    success_batch = success_df.limit(1).collect()\n",
    "\n",
    "    if failed_batch and success_batch:\n",
    "        failed_version = failed_batch[0][\"delta_table_version\"]\n",
    "        success_version = success_batch[0][\"delta_table_version\"]\n",
    "        failed_batch_id = failed_batch[0][\"batch_id\"]\n",
    "\n",
    "        if failed_version > success_version:\n",
    "            print(f\"⚠️ Triggering rollback: FAILED version {failed_version} > SUCCESS version {success_version}\")\n",
    "            rollback_batch(failed_batch_id, success_version)\n",
    "        else:\n",
    "            print(f\"✅ No rollback needed: FAILED version {failed_version} ≤ SUCCESS version {success_version}\")\n",
    "    else:\n",
    "        print(\"ℹ️ No rollback triggered — missing FAILED or SUCCESS batch in audit table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7a1f1d-80e4-47ae-88d9-2839d9c4689a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_failure_to_audit(\n",
    "    batch_id: str\n",
    "):\n",
    "    current_version = get_delta_version(target_path)\n",
    "\n",
    "    # Load the audit Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, audit_path)\n",
    "\n",
    "    # Update status to SUCCESS for the given batch_id\n",
    "    delta_table.update(\n",
    "        condition = \"batch_id = '{}'\".format(batch_id),\n",
    "        set = { \"status\": \"'FAILED'\", \"rollback_flag\": \"'N'\", \"delta_table_version\": f\"{current_version}\" }\n",
    "    )\n",
    "    check_and_trigger_rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b95d99f-08de-4872-824b-5eabbe97a7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_success_to_audit(\n",
    "    batch_id: str\n",
    "):\n",
    "    current_version = get_delta_version(target_path)\n",
    "\n",
    "    # Load the audit Delta table\n",
    "    delta_table = DeltaTable.forPath(spark, audit_path)\n",
    "\n",
    "    # Update status to SUCCESS for the given batch_id\n",
    "    delta_table.update(\n",
    "        condition = \"batch_id = '{}'\".format(batch_id),\n",
    "        set = { \"status\": \"'SUCCESS'\", \"rollback_flag\": \"'N'\", \"delta_table_version\": f\"{current_version}\" }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3dfaf1-113e-43c3-95d8-9476a2bf8d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "json = 'abfss://gold@rmpyru.dfs.core.windows.net'\n",
    "df_zone = spark.read.format('json')\\\n",
    "                .option('inferSchema',True)\\\n",
    "                .option('multiLine',True)\\\n",
    "                .option('header',True)\\\n",
    "                .load(f'{json}/resturant_json_data.json')               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12dd2922-5170-48fe-ad37-7ee9623dd3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Metadata\n",
    "batch_id = str(uuid.uuid4())\n",
    "timestamp = datetime.now()\n",
    "source_path = \"abfss://gold@rmpyru.dfs.core.windows.net/resturant_json_data.json\"\n",
    "record_count = df_zone.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000a70f1-0e53-44ff-a3ff-4dbfc843c521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_df = spark.createDataFrame([(\n",
    "    batch_id,\n",
    "    timestamp,\n",
    "    source_path,\n",
    "    record_count,\n",
    "    current_version,\n",
    "    \"STARTED\",\n",
    "    \"N\"\n",
    ")], schema=metadata_schema)\n",
    "metadata_df.write.format(\"delta\").mode(\"append\").save(audit_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f6543f-6a0a-49eb-a716-982eb61fe0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"restaurant name\",col(\"restaurants.restaurant.name\"))\\\n",
    "            .withColumn(\"cuisines\",col(\"restaurants.restaurant.cuisines\"))\\\n",
    "                .withColumn(\"ratings\",col(\"restaurants.restaurant.user_rating.rating_text\"))\\\n",
    "                    .withColumn(\"city\",col(\"restaurants.restaurant.location.city\"))\\\n",
    "                        .withColumn(\"establishment_types\",explode_outer(col(\"restaurants.restaurant.establishment_types\")))\\\n",
    "                            .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\")\\\n",
    "                                .filter(col(\"city\")==\"Columbus\")\\\n",
    "                                    .groupBy(\"ratings\").count().alias(\"restaurant_ratings\")\n",
    "                                \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d450849d-d23f-496c-a07d-5cc28c3e4b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_restaurant=df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"restaurant name\",col(\"restaurants.restaurant.name\"))\\\n",
    "            .withColumn(\"city\",col(\"restaurants.restaurant.location.city\"))\\\n",
    "                .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\",\"restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1963a575-6ca5-4aba-a83a-26bf3e15d925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_restaurant_rating=df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"ratings\",col(\"restaurants.restaurant.user_rating.rating_text\"))\\\n",
    "            .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\",\"restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289d11e2-1dd3-4457-b9f1-8aef73ac20cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_restaurant_cuisines=df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"cuisines\",col(\"restaurants.restaurant.cuisines\"))\\\n",
    "            .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\",\"restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fd9887-2fc6-4bfe-a278-4e800f1b49d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_restaurant.display()\n",
    "#df_restaurant_rating.cache()\n",
    "#df_restaurant_cuisines.display()\n",
    "\n",
    "df_final = df_restaurant.join(broadcast(df_restaurant_rating),df_restaurant[\"restaurant id\"]==df_restaurant_rating[\"restaurant id\"],how=\"left\").join(df_restaurant_cuisines,df_restaurant[\"restaurant id\"]==df_restaurant_cuisines[\"restaurant id\"],how=\"inner\").filter((col(\"restaurant name\")!=\"\") & (col(\"cuisines\")==\"\")).select(df_restaurant[\"restaurant id\"],\"restaurant name\",\"ratings\",\"cuisines\").groupBy(\"ratings\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5497f89-e6a3-40b5-866b-3fcf8efe72c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_final.partitionBy(\"ratings\")\n",
    "#df_final.write.mode(\"overwrite\").format(\"delta\").save(f'{json}/zomato')\n",
    "try:\n",
    "    df_final.write.partitionBy(\"ratings\").mode(\"overwrite\").format(\"delta\").save(target_path)\n",
    "    log_success_to_audit(batch_id=batch_id)\n",
    "except Exception as e:\n",
    "    log_failure_to_audit(batch_id=batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f64e3a-ff7b-4c3f-95aa-5baae74f6730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_final = df_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1ad4a4-d047-4ef6-a07d-7ef9a9dd2416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from delta.`abfss://gold@rmpyru.dfs.core.windows.net/zomato`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa6febf-1b2b-42a2-b23a-9d10ea5ae5c1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"batch_id\":351,\"source_path\":415,\"timestamp\":253},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754024641748}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from delta.`abfss://gold@rmpyru.dfs.core.windows.net/Audit`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7397876439054100,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "RollBackUtility",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
