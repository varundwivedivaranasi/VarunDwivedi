{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0700db19-8a07-4068-be88-3a86222634d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee3c3fd-c3b1-42f4-bd79-3c3626e38c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"batch_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"source_path\", StringType(), True),\n",
    "    StructField(\"record_count\", LongType(), True),\n",
    "    StructField(\"delta_table_version\", LongType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"rollback_flag\", StringType(), True)  # 'Y' or 'N'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b14f90-8508-4e89-bc9a-03bdb98ea083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from datetime import datetime\n",
    "\n",
    "def log_failure_to_audit(\n",
    "    batch_id: str,\n",
    "    source_path: str,\n",
    "    record_count: int,\n",
    "    delta_table_version: int = None\n",
    "):\n",
    "\n",
    "\n",
    "    metadata_row = Row(\n",
    "        batch_id=batch_id,\n",
    "        timestamp=datetime.now(),\n",
    "        source_path=source_path,\n",
    "        record_count=record_count,\n",
    "        delta_table_version=delta_table_version,\n",
    "        status=\"FAILED\",\n",
    "        rollback_flag=\"N\"\n",
    "    )\n",
    "\n",
    "    metadata_df = spark.createDataFrame([metadata_row])\n",
    "    metadata_df.write.format(\"delta\").mode(\"append\").save(\"abfss://gold@rmpyru.dfs.core.windows.net/Audit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b95d99f-08de-4872-824b-5eabbe97a7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_success_to_audit(\n",
    "    batch_id: str\n",
    "):\n",
    "    current_version = get_delta_version(target_path)\n",
    "\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # Load the audit Delta table\n",
    "    audit_path = \"abfss://gold@rmpyru.dfs.core.windows.net/Audit\"\n",
    "    delta_table = DeltaTable.forPath(spark, audit_path)\n",
    "\n",
    "    # Update status to SUCCESS for the given batch_id\n",
    "    delta_table.update(\n",
    "        condition = \"batch_id = '{}'\".format(batch_id),\n",
    "        set = { \"status\": \"'SUCCESS'\", \"rollback_flag\": \"'N'\", \"delta_table_version\": f\"{current_version}\" }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3dfaf1-113e-43c3-95d8-9476a2bf8d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "json = 'abfss://gold@rmpyru.dfs.core.windows.net'\n",
    "df_zone = spark.read.format('json')\\\n",
    "                .option('inferSchema',True)\\\n",
    "                .option('multiLine',True)\\\n",
    "                .option('header',True)\\\n",
    "                .load(f'{json}/resturant_json_data.json')               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12dd2922-5170-48fe-ad37-7ee9623dd3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType\n",
    "from delta.tables import DeltaTable\n",
    "import os\n",
    "\n",
    "# Metadata\n",
    "batch_id = str(uuid.uuid4())\n",
    "timestamp = datetime.now()\n",
    "source_path = \"abfss://gold@rmpyru.dfs.core.windows.net/resturant_json_data.json\"\n",
    "record_count = df_zone.count()\n",
    "\n",
    "# Target path\n",
    "target_path = \"abfss://gold@rmpyru.dfs.core.windows.net/zomato\"\n",
    "\n",
    "# Function to safely get Delta version\n",
    "def get_delta_version(path):\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, path)\n",
    "        history_df = delta_table.history()\n",
    "        if history_df.count() > 0:\n",
    "            return history_df.head(1)[0]['version']\n",
    "        else:\n",
    "            print(\"Delta table exists but has no version history.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Delta table not found at {path}. Initializing...\")\n",
    "        return None\n",
    "\n",
    "# Check and initialize if needed\n",
    "current_version = get_delta_version(target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000a70f1-0e53-44ff-a3ff-4dbfc843c521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_df = spark.createDataFrame([(\n",
    "    batch_id,\n",
    "    timestamp,\n",
    "    source_path,\n",
    "    record_count,\n",
    "    current_version,\n",
    "    \"STARTED\",\n",
    "    \"N\"\n",
    ")], schema=metadata_schema)\n",
    "metadata_df.write.format(\"delta\").mode(\"append\").save(\"abfss://gold@rmpyru.dfs.core.windows.net/Audit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f6543f-6a0a-49eb-a716-982eb61fe0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"restaurant name\",col(\"restaurants.restaurant.name\"))\\\n",
    "            .withColumn(\"cuisines\",col(\"restaurants.restaurant.cuisines\"))\\\n",
    "                .withColumn(\"ratings\",col(\"restaurants.restaurant.user_rating.rating_text\"))\\\n",
    "                    .withColumn(\"city\",col(\"restaurants.restaurant.location.city\"))\\\n",
    "                        .withColumn(\"establishment_types\",explode_outer(col(\"restaurants.restaurant.establishment_types\")))\\\n",
    "                            .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\")\\\n",
    "                                .filter(col(\"city\")==\"Columbus\")\\\n",
    "                                    .groupBy(\"ratings\").count().alias(\"restaurant_ratings\")\n",
    "                                \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d450849d-d23f-496c-a07d-5cc28c3e4b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_restaurant=df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"restaurant name\",col(\"restaurants.restaurant.name\"))\\\n",
    "            .withColumn(\"city\",col(\"restaurants.restaurant.location.city\"))\\\n",
    "                .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\",\"restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1963a575-6ca5-4aba-a83a-26bf3e15d925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_restaurant_rating=df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"ratings\",col(\"restaurants.restaurant.user_rating.rating_text\"))\\\n",
    "            .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\",\"restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289d11e2-1dd3-4457-b9f1-8aef73ac20cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_restaurant_cuisines=df_zone.withColumn(\"restaurants\",explode(\"restaurants\"))\\\n",
    "    .withColumn(\"restaurant id\",col(\"restaurants.restaurant.id\"))\\\n",
    "        .withColumn(\"cuisines\",col(\"restaurants.restaurant.cuisines\"))\\\n",
    "            .drop(\"code\",\"message\",\"results_found\",\"results_shown\",\"results_start\",\"status\",\"restaurants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fd9887-2fc6-4bfe-a278-4e800f1b49d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_restaurant.display()\n",
    "#df_restaurant_rating.cache()\n",
    "#df_restaurant_cuisines.display()\n",
    "\n",
    "df_final = df_restaurant.join(broadcast(df_restaurant_rating),df_restaurant[\"restaurant id\"]==df_restaurant_rating[\"restaurant id\"],how=\"left\").join(df_restaurant_cuisines,df_restaurant[\"restaurant id\"]==df_restaurant_cuisines[\"restaurant id\"],how=\"inner\").filter((col(\"restaurant name\")!=\"\") & (col(\"cuisines\")==\"\")).select(df_restaurant[\"restaurant id\"],\"restaurant name\",\"ratings\",\"cuisines\").groupBy(\"ratings\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f64e3a-ff7b-4c3f-95aa-5baae74f6730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_final = df_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca82a81-2714-4d19-8c1d-6960fd922298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df_final.partitionBy(\"ratings\")\n",
    "#df_final.write.mode(\"overwrite\").format(\"delta\").save(f'{json}/zomato')\n",
    "try:\n",
    "    df_final.write.partitionBy(\"ratings\").mode(\"overwrite\").format(\"delta\").save(f'{json}/zomato')\n",
    "    log_success_to_audit(batch_id=batch_id)\n",
    "except Exception as e:\n",
    "    log_failure_to_audit(\n",
    "        batch_id=batch_id,\n",
    "        source_path=source_path,\n",
    "        record_count=df_zone.count(),\n",
    "        delta_table_version=current_version,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95929ba-e6f7-4531-8cc8-93debd6e7afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rollback_batch(batch_id: str):\n",
    "    audit_df = spark.read.format(\"delta\").load(\"abfss://gold@rmpyru.dfs.core.windows.net/Audit\")\n",
    "    rollback_info = audit_df.filter(f\"batch_id = '{batch_id}'\").orderBy(\"timestamp\", ascending=False).limit(1).collect()[0]\n",
    "    \n",
    "    rollback_version = rollback_info[\"delta_table_version\"]\n",
    "    restored_df = spark.read.format(\"delta\").option(\"versionAsOf\", rollback_version).load(\"abfss://gold@rmpyru.dfs.core.windows.net/zomato\")\n",
    "    \n",
    "    restored_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://gold@rmpyru.dfs.core.windows.net/zomato\")\n",
    "    #print(f\"Rollback to version {rollback_version} completed for batch {batch_id}\")\n",
    "    current_version = get_delta_version(target_path)\n",
    "\n",
    "    updated_metadata_df = spark.createDataFrame([(\n",
    "        batch_id,\n",
    "        timestamp,\n",
    "        source_path,\n",
    "        record_count,\n",
    "        current_version,\n",
    "        \"ROLLEDBACK\",\n",
    "        \"Y\"\n",
    "    )], schema=metadata_schema)\n",
    "\n",
    "    updated_metadata_df.write.format(\"delta\").mode(\"append\").save(\"abfss://gold@rmpyru.dfs.core.windows.net/Audit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76363d1-e112-4b72-868b-01870c510bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_df = spark.read.format(\"delta\").load(\"abfss://gold@rmpyru.dfs.core.windows.net/Audit\")\n",
    "\n",
    "# Get latest failed batch (assuming you log status)\n",
    "latest_failed_batch = (\n",
    "    audit_df.filter(\"status = 'FAILED'\")\n",
    "            .orderBy(\"timestamp\", ascending=False)\n",
    "            .select(\"batch_id\")\n",
    "            .head(1)[0][\"batch_id\"] if audit_df.filter(\"status = 'FAILED'\").count() > 0 else None\n",
    ")\n",
    "\n",
    "if latest_failed_batch is not None:    rollback_batch(latest_failed_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1ad4a4-d047-4ef6-a07d-7ef9a9dd2416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from delta.`abfss://gold@rmpyru.dfs.core.windows.net/zomato`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa6febf-1b2b-42a2-b23a-9d10ea5ae5c1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"batch_id\":351,\"source_path\":415,\"timestamp\":253},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754024641748}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from delta.`abfss://gold@rmpyru.dfs.core.windows.net/Audit`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7397876439054100,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "RollBackUtility",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
