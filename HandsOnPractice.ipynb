{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e168ad-32a5-4d47-928b-1d7898fada55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    " (\"C001\", \"2024-01-01\"),\n",
    " (\"C001\", \"2024-01-04\"),\n",
    " (\"C001\", \"2024-01-06\"),\n",
    " (\"C002\", \"2024-01-03\"),\n",
    " (\"C002\", \"2024-01-05\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"billing_date\"])\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2210bd44-f731-48ef-abaf-9a5f5a94ee8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, datediff, date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7505a8-d369-407d-a9b7-5b9006a5ab92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_window=Window.partitionBy(\"customer_id\").orderBy(\"billing_date\")\n",
    "\n",
    "prev_date_df=df.withColumn(\"prev_date\",lag(\"billing_date\",1).over(customer_window))\n",
    "\n",
    "prev_date_df.display()\n",
    "\n",
    "gap_df=prev_date_df.filter(datediff(col(\"billing_date\"),col(\"prev_date\"))>1)\n",
    "\n",
    "gap_df.display()\n",
    "\n",
    "result_df=gap_df.withColumn(\"missing_from\",date_add(col(\"prev_date\"),1)).withColumn(\"missing_to\",date_sub(col('billing_date'),1))\n",
    "\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e400a4-6e66-4ad8-a012-0c0a6617045d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, explode, sequence, to_date\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"MissingBillingDates\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"C001\", \"2024-01-01\"),\n",
    "    (\"C001\", \"2024-01-02\"),\n",
    "    (\"C001\", \"2024-01-04\"),\n",
    "    (\"C001\", \"2024-01-06\"),\n",
    "    (\"C002\", \"2024-01-03\"),\n",
    "    (\"C002\", \"2024-01-05\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"billing_date\"]) \\\n",
    "    .withColumn(\"billing_date\", to_date(\"billing_date\"))\n",
    "df.display()\n",
    "# Step 1: Get min and max billing date per customer\n",
    "date_range_df = df.groupBy(\"customer_id\").agg(\n",
    "    min(\"billing_date\").alias(\"start_date\"),\n",
    "    max(\"billing_date\").alias(\"end_date\")\n",
    ")\n",
    "date_range_df.display()\n",
    "# Step 2: Generate complete date range per customer\n",
    "full_dates_df = date_range_df.withColumn(\"billing_date\", explode(\n",
    "    sequence(col(\"start_date\"), col(\"end_date\"))\n",
    ")).select(\"customer_id\", \"billing_date\")\n",
    "full_dates_df.display()\n",
    "# Step 3: Find missing dates via anti-join\n",
    "missing_dates_df = full_dates_df.join(df, on=[\"customer_id\", \"billing_date\"], how=\"anti\")\n",
    "# Show results\n",
    "missing_dates_df.orderBy(\"customer_id\", \"billing_date\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f6198f-964c-450e-8a18-b0dbbb844944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#input: data = [(1,), (2,), (4,), (6,), (8,), (9,)]\n",
    "\n",
    "#output expected : [(3,), (5,), (7,)]\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Actual numbers DataFrame\n",
    "data = [(1,), (2,), (4,), (6,), (8,), (9,)]\n",
    "numbers_df = spark.createDataFrame(data, [\"num\"])\n",
    "\n",
    "#Tip1 > create dataframe having values 1 to 9\n",
    "\n",
    "# Full range DataFrame (1 to 9)\n",
    "full_range_df = spark.range(1, 10).toDF(\"num\")\n",
    "\n",
    "# Use anti join to find missing numbers\n",
    "missing_numbers = full_range_df.join(numbers_df, on=\"num\", how=\"left_anti\")\n",
    "missing_numbers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "802318e1-9919-437f-b5c0-31e215301197",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753507205445}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [\n",
    "    (101,[\"P1\",\"P2\"]),\n",
    "    (102,[\"P1\"]),\n",
    "    (103,[\"P2\",\"P3\"])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,[\"id\",\"projects\"])\n",
    "df.display()\n",
    "\n",
    "df_splitedlist = df.withColumn(\"projects\", explode(\"projects\"))\n",
    "df_splitedlist.display()\n",
    "\n",
    "df_result = df_splitedlist.groupBy(\"projects\").agg(countDistinct(\"id\").alias(\"count\"))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aece0d6c-24b9-4bdc-b4e6-8266a43b2b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Setup\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample sales data\n",
    "data = [\n",
    "    (\"ProductA\", \"2024-01\", 100),\n",
    "    (\"ProductA\", \"2024-02\", 120),\n",
    "    (\"ProductA\", \"2024-03\", 90),\n",
    "    (\"ProductB\", \"2024-01\", 200),\n",
    "    (\"ProductB\", \"2024-02\", 210),\n",
    "    (\"ProductB\", \"2024-03\", 200),\n",
    "]\n",
    "\n",
    "columns = [\"Product\", \"Month\", \"Revenue\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5015a8f-ccdd-49ee-b650-43a7de3e3413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window partitioned by product and ordered by month\n",
    "windowSpec = Window.partitionBy(\"Product\").orderBy(\"Month\")\n",
    "\n",
    "# Add lag and lead columns\n",
    "df_with_lag_lead = df.withColumn(\"Prev_Revenue\", lag(\"Revenue\", 1).over(windowSpec)) \\\n",
    "                     .withColumn(\"Next_Revenue\", lead(\"Revenue\", 1).over(windowSpec))\n",
    "\n",
    "df_with_lag_lead.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639d6923-cef6-40d5-9ab6-b887a75781d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn('Flag', lit('Flag'))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "439ddb79-53d3-4a1d-9993-8d0d1c7b4239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn('Flag1', lit('Flag'))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05245bca-d0d4-414e-9de9-64e66b79c077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_lag_lead_wri.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"rro.sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f28385-7ee6-4789-bef6-998b80efaa92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rro.sales_data version as of 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9ace37-fd88-466a-9b6b-c886af064323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "RESTORE TABLE rro.sales_data TO VERSION AS OF 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bef3dd8-3be0-4964-951d-80e44645de10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_sqldf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c288c09-d399-4ac4-a6af-5311c30263ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753539794768}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rro.sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ad1188-da25-4aea-93d5-cd0d35923dfd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753540349429}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn('Flag', lit('Flag')).withColumn('Flag1', lit('Flag')).withColumn('Flag2', lit('Flag'))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0fa4b60-8eeb-415d-8d6d-cc2faf002236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_lag_lead_wri.select(\"Product\",\"Month\",\"Revenue\",\"Prev_Revenue\",\"Next_Revenue\",\"Revenue_Gap\",\"Flag\",\"Flag1\").write.mode(\"append\").saveAsTable(\"rro.sales_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "HandsOnPractice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
