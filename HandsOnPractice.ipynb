{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44e168ad-32a5-4d47-928b-1d7898fada55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    " (\"C001\", \"2024-01-01\"),\n",
    " (\"C001\", \"2024-01-04\"),\n",
    " (\"C001\", \"2024-01-06\"),\n",
    " (\"C002\", \"2024-01-03\"),\n",
    " (\"C002\", \"2024-01-05\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"billing_date\"])\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2210bd44-f731-48ef-abaf-9a5f5a94ee8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, datediff, date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7505a8-d369-407d-a9b7-5b9006a5ab92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_window=Window.partitionBy(\"customer_id\").orderBy(\"billing_date\")\n",
    "\n",
    "prev_date_df=df.withColumn(\"prev_date\",lag(\"billing_date\",1).over(customer_window))\n",
    "\n",
    "prev_date_df.display()\n",
    "\n",
    "gap_df=prev_date_df.filter(datediff(col(\"billing_date\"),col(\"prev_date\"))>1)\n",
    "\n",
    "gap_df.display()\n",
    "\n",
    "result_df=gap_df.withColumn(\"missing_from\",date_add(col(\"prev_date\"),1)).withColumn(\"missing_to\",date_sub(col('billing_date'),1))\n",
    "\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e400a4-6e66-4ad8-a012-0c0a6617045d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, explode, sequence, to_date\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"MissingBillingDates\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"C001\", \"2024-01-01\"),\n",
    "    (\"C001\", \"2024-01-02\"),\n",
    "    (\"C001\", \"2024-01-04\"),\n",
    "    (\"C001\", \"2024-01-06\"),\n",
    "    (\"C002\", \"2024-01-03\"),\n",
    "    (\"C002\", \"2024-01-05\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"billing_date\"]) \\\n",
    "    .withColumn(\"billing_date\", to_date(\"billing_date\"))\n",
    "df.display()\n",
    "# Step 1: Get min and max billing date per customer\n",
    "date_range_df = df.groupBy(\"customer_id\").agg(\n",
    "    min(\"billing_date\").alias(\"start_date\"),\n",
    "    max(\"billing_date\").alias(\"end_date\")\n",
    ")\n",
    "date_range_df.display()\n",
    "# Step 2: Generate complete date range per customer\n",
    "full_dates_df = date_range_df.withColumn(\"billing_date\", explode(\n",
    "    sequence(col(\"start_date\"), col(\"end_date\"))\n",
    ")).select(\"customer_id\", \"billing_date\")\n",
    "full_dates_df.display()\n",
    "# Step 3: Find missing dates via anti-join\n",
    "missing_dates_df = full_dates_df.join(df, on=[\"customer_id\", \"billing_date\"], how=\"anti\")\n",
    "# Show results\n",
    "missing_dates_df.orderBy(\"customer_id\", \"billing_date\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f6198f-964c-450e-8a18-b0dbbb844944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#input: data = [(1,), (2,), (4,), (6,), (8,), (9,)]\n",
    "\n",
    "#output expected : [(3,), (5,), (7,)]\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Actual numbers DataFrame\n",
    "data = [(1,), (2,), (4,), (6,), (8,), (9,)]\n",
    "numbers_df = spark.createDataFrame(data, [\"num\"])\n",
    "\n",
    "#Tip1 > create dataframe having values 1 to 9\n",
    "\n",
    "# Full range DataFrame (1 to 9)\n",
    "full_range_df = spark.range(1, 10).toDF(\"num\")\n",
    "\n",
    "# Use anti join to find missing numbers\n",
    "missing_numbers = full_range_df.join(numbers_df, on=\"num\", how=\"left_anti\")\n",
    "missing_numbers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "802318e1-9919-437f-b5c0-31e215301197",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753507205445}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [\n",
    "    (101,[\"P1\",\"P2\"]),\n",
    "    (102,[\"P1\"]),\n",
    "    (103,[\"P2\",\"P3\"])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,[\"id\",\"projects\"])\n",
    "df.display()\n",
    "\n",
    "df_splitedlist = df.withColumn(\"projects\", explode(\"projects\"))\n",
    "df_splitedlist.display()\n",
    "\n",
    "df_result = df_splitedlist.groupBy(\"projects\").agg(countDistinct(\"id\").alias(\"count\"))\n",
    "df_result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aece0d6c-24b9-4bdc-b4e6-8266a43b2b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Setup\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample sales data\n",
    "data = [\n",
    "    (\"ProductA\", \"2024-01\", 100),\n",
    "    (\"ProductA\", \"2024-02\", 120),\n",
    "    (\"ProductA\", \"2024-03\", 90),\n",
    "    (\"ProductB\", \"2024-01\", 200),\n",
    "    (\"ProductB\", \"2024-02\", 210),\n",
    "    (\"ProductB\", \"2024-03\", 200),\n",
    "]\n",
    "\n",
    "columns = [\"Product\", \"Month\", \"Revenue\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5015a8f-ccdd-49ee-b650-43a7de3e3413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window partitioned by product and ordered by month\n",
    "windowSpec = Window.partitionBy(\"Product\").orderBy(\"Month\")\n",
    "\n",
    "# Add lag and lead columns\n",
    "df_with_lag_lead = df.withColumn(\"Prev_Revenue\", lag(\"Revenue\", 1).over(windowSpec)) \\\n",
    "                     .withColumn(\"Next_Revenue\", lead(\"Revenue\", 1).over(windowSpec))\n",
    "\n",
    "df_with_lag_lead.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639d6923-cef6-40d5-9ab6-b887a75781d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn('Flag', lit('Flag'))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "439ddb79-53d3-4a1d-9993-8d0d1c7b4239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn('Flag1', lit('Flag'))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05245bca-d0d4-414e-9de9-64e66b79c077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_lag_lead_wri.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"rro.sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f28385-7ee6-4789-bef6-998b80efaa92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rro.sales_data version as of 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9ace37-fd88-466a-9b6b-c886af064323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "RESTORE TABLE rro.sales_data TO VERSION AS OF 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bef3dd8-3be0-4964-951d-80e44645de10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_sqldf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c288c09-d399-4ac4-a6af-5311c30263ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753539794768}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rro.sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ad1188-da25-4aea-93d5-cd0d35923dfd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753540349429}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "\n",
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn('Flag', lit('Flag'))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0a46a3-0ef0-40c1-8a46-cd7a733516a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "required_columns = [\"Product\", \"Month\", \"Revenue\", \"Prev_Revenue\", \"Next_Revenue\", \"Revenue_Gap\", \"Flag\", \"Flag1\"]\n",
    "available_columns = df_with_lag_lead_wri.columns\n",
    "print(available_columns)\n",
    "valid_columns = [col for col in required_columns if col in available_columns]\n",
    "print(valid_columns)\n",
    "df_with_lag_lead_wri.select(*valid_columns).write.mode(\"overwrite\").saveAsTable(\"rro.sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec761632-a241-4bbf-9214-95d342e0b2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rro.sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0fa4b60-8eeb-415d-8d6d-cc2faf002236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_with_lag_lead_wri.select(\"Product\",\"Month\",\"Revenue\",\"Prev_Revenue\",\"Next_Revenue\",\"Revenue_Gap\",\"Flag\",\"Flag1\").write.mode(\"append\").saveAsTable(\"rro.sales_data\")\n",
    "except Exception as e:\n",
    "  print(\"An error ocured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d77068d-cc28-4bbd-a452-0a9637250e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from rro.sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7633e6a-8012-4f2a-9e7c-4da93827797b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def validate_schema(df: DataFrame, expected_columns: list, strict: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Validates that all expected_columns exist in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Spark DataFrame to check\n",
    "    - expected_columns: List of required column names\n",
    "    - strict: If True, also checks that no extra columns exist\n",
    "    \n",
    "    Returns:\n",
    "    - True if schema matches; False otherwise\n",
    "    \"\"\"\n",
    "    actual_columns = set(df.columns)\n",
    "    expected_set = set(expected_columns)\n",
    "    \n",
    "    missing = expected_set - actual_columns\n",
    "    extras = actual_columns - expected_set if strict else set()\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"[âŒ] Missing columns: {sorted(missing)}\")\n",
    "    if extras:\n",
    "        print(f\"[âš ï¸] Unexpected columns (strict mode): {sorted(extras)}\")\n",
    "    \n",
    "    return not missing and not extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44725bac-4e7a-494a-956c-a33a7181d1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_cols = [\"Product\", \"Month\", \"Revenue\", \"Prev_Revenue\", \n",
    "                 \"Next_Revenue\", \"Revenue_Gap\", \"Flag\", \"Flag1\"]\n",
    "\n",
    "is_valid = validate_schema(df_with_lag_lead_wri, expected_cols, strict=False)\n",
    "\n",
    "if is_valid:\n",
    "    df_with_lag_lead_wri.select(*expected_cols).write.mode(\"append\").saveAsTable(\"rro_sales_data\")\n",
    "else:\n",
    "    print(\"[ðŸš«] Schema mismatch. Fix required before write operation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb2f117-15b3-4dc2-b3ec-6da5bd2b5d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Fetch schema from rro_sales_data\n",
    "table_schema = spark.table(\"rro.sales_data\").schema\n",
    "display(table_schema)\n",
    "# Extract column names\n",
    "expected_cols = [field.name for field in table_schema]\n",
    "display(expected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "270db8aa-6054-447d-82af-81998d7f1d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "is_valid = validate_schema(df_with_lag_lead_wri, expected_cols, strict=False)\n",
    "\n",
    "if is_valid:\n",
    "    df_with_lag_lead_wri.select(*expected_cols).write.mode(\"append\").saveAsTable(\"rro_sales_data\")\n",
    "else:\n",
    "    print(\"[âŒ] Schema mismatch. Fix required before write operation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c40a91-bf94-4dd3-a61c-0e9e3871f47d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = [(1, None), (2, 3.0), (None, 4.0), (4, 5.0)]\n",
    "columns = [\"col1\", \"col2\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Calculate column-wise means (excluding nulls)\n",
    "means = df.select([mean(col(c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "\n",
    "# Fill nulls with computed means\n",
    "df_filled = df.fillna(means)\n",
    "\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaad85b9-1732-4942-a42d-bc7d1afb736a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"MultiColumnJoin\").getOrCreate()\n",
    "\n",
    "# Sample DataFrames\n",
    "data1 = [(1, \"A\", 100), (2, \"B\", 200), (3, \"C\", 300)]\n",
    "data2 = [(1, \"A\", \"X\"), (2, \"B\", \"Y\"), (4, \"D\", \"Z\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"id\", \"code\", \"value\"])\n",
    "df2 = spark.createDataFrame(data2, [\"id\", \"code\", \"flag\"])\n",
    "\n",
    "# Perform inner join on multiple columns: 'id' and 'code'\n",
    "joined_df1 = df1.join(df2, on=[\"id\", \"code\"], how=\"inner\")\n",
    "joined_df2 = df1.join(broadcast(df2), (df1.id==df2.id) & (df1.code==df2.code), how=\"inner\")\n",
    "\n",
    "# Show result\n",
    "joined_df1.show()\n",
    "joined_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df727a8e-4f99-4b57-831c-2c85d8dfd477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"MovingAverage\").getOrCreate()\n",
    "\n",
    "# Sample data (e.g., daily sales)\n",
    "data = [\n",
    "    (\"2023-01-01\", 100),\n",
    "    (\"2023-01-02\", 150),\n",
    "    (\"2023-01-03\", 200),\n",
    "    (\"2023-01-04\", 250),\n",
    "    (\"2023-01-05\", 300)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date\", \"sales\"])\n",
    "\n",
    "# Define a rolling window â€” past 3 rows including current\n",
    "window_spec = Window.orderBy(\"date\").rowsBetween(-2, 0)\n",
    "\n",
    "# Calculate moving average\n",
    "df_with_ma = df.withColumn(\"moving_avg\", avg(col(\"sales\")).over(window_spec))\n",
    "\n",
    "df_with_ma.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b3d002-5c75-4eb9-b203-9eaf5f75e404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesRanking\").getOrCreate()\n",
    "\n",
    "# Sample sales transaction data\n",
    "data = [\n",
    "    (\"North\", \"CustA\", 500),\n",
    "    (\"North\", \"CustA\", 700),\n",
    "    (\"North\", \"CustB\", 600),\n",
    "    (\"South\", \"CustA\", 800),\n",
    "    (\"South\", \"CustB\", 750),\n",
    "    (\"South\", \"CustB\", 900)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"region\", \"customer_id\", \"sales\"])\n",
    "\n",
    "# Define window spec to partition by region and customer, order by sales DESC\n",
    "window_spec = Window.partitionBy(\"region\", \"customer_id\").orderBy(col(\"sales\").desc())\n",
    "\n",
    "# Add rank column\n",
    "ranked_df = df.withColumn(\"sales_rank\", rank().over(window_spec))\n",
    "\n",
    "# Show result\n",
    "ranked_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33766bf8-41b9-4c73-a6fc-4064bea0b0b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"FilterSingleOccurrences\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"CustA\", 100),\n",
    "    (\"CustB\", 200),\n",
    "    (\"CustA\", 150),\n",
    "    (\"CustC\", 300),\n",
    "    (\"CustD\", 400)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"sales\"])\n",
    "\n",
    "# Step 1: Identify values that appear more than once\n",
    "duplicates = df.groupBy(\"customer_id\") \\\n",
    "               .agg(count(\"*\").alias(\"cnt\")) \\\n",
    "               .filter(col(\"cnt\") > 1) \\\n",
    "               .select(\"customer_id\")\n",
    "duplicates.show()\n",
    "# Step 2: Filter original DataFrame using semi join\n",
    "filtered_df = df.join(duplicates, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c5a24c-f60d-49e0-aae2-f1a66e03db2e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"text\":364},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753889704466}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, lower, col, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"TopNWords\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame with text\n",
    "data = [(\"Hello world spark spark\",), (\"Spark is fast and powerful\",), (\"Hello from the other side\",)]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "df.display()\n",
    "# Step 1: Normalize and split text into words\n",
    "words_df = df.select(explode(split(lower(col(\"text\")), \" \")).alias(\"word\"))\n",
    "words_df.display()\n",
    "# Step 2: Count frequency of each word\n",
    "word_counts = words_df.groupBy(\"word\").agg(count(\"*\").alias(\"freq\"))\n",
    "word_counts.display()\n",
    "# Step 3: Order and extract top N\n",
    "top_n = word_counts.orderBy(col(\"freq\").desc()).limit(5)  # Change 5 to your desired N\n",
    "\n",
    "top_n.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7089e625-22bc-4e1b-8743-274e0392ae66",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"text\":345},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753889916315}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, lower, col, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"TopNWords\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame with text\n",
    "data = [(\"Hello world spark spark\",), (\"Spark is fast and powerful\",), (\"Hello from the other side\",)]\n",
    "df = spark.createDataFrame(data, [\"text\"])\n",
    "df.display()\n",
    "\n",
    "df_explode = df.withColumn(\"word\",explode(split(col(\"text\"),\" \"))).drop(\"text\")\n",
    "\n",
    "df_count = df_explode.groupBy(\"word\").agg(count(\"*\").alias(\"freq\")).orderBy(col(\"freq\").desc()).limit(5)\n",
    "\n",
    "df_count.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b34be5-ac10-4e07-b2e4-60cfefcd3516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "# Define your window spec\n",
    "window_spec = Window.partitionBy(\"user_id\", \"product_id\").orderBy(col(\"event_time\").desc())\n",
    "\n",
    "# Add row number within each group\n",
    "df_ranked = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only latest record per group\n",
    "df_deduped = df_ranked.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "df_deduped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76af449-eccb-484d-b4e1-cfcb3d4be196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import least, greatest\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Mumbai\",\"Pune\"),\n",
    "        (\"Pune\",\"Mumbai\"),\n",
    "        (\"Delhi\",\"Bangalore\"),\n",
    "        (\"Bangalore\",\"Delhi\")]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"source\", \"destination\"])\n",
    "\n",
    "# Step 1: build canonical columns\n",
    "df2 = df.withColumn(\"src_key\", least(\"source\", \"destination\")) \\\n",
    "        .withColumn(\"dst_key\", greatest(\"source\", \"destination\"))\n",
    "df2.display()\n",
    "# Step 2: drop cross-duplicates\n",
    "deduped = df2.dropDuplicates([\"src_key\", \"dst_key\"]) \\\n",
    "             .select(\"source\", \"destination\")\n",
    "\n",
    "deduped.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48314d5d-b25b-4077-accc-1e3195d31fb3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753958982225}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753960558388}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_date, row_number, count, min, max, date_sub\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, '2025-03-01'), (1, '2025-03-02'), (1, '2025-03-03'),\n",
    "    (1, '2025-03-04'), (1, '2025-03-06'), (1, '2025-03-10'),\n",
    "    (1, '2025-03-11'), (1, '2025-03-12'), (1, '2025-03-13'),\n",
    "    (1, '2025-03-14'), (1, '2025-03-25'), (1, '2025-03-26'),\n",
    "    (1, '2025-03-27'), (1, '2025-03-28'), (1, '2025-03-29'),\n",
    "    (1, '2025-03-30'), (2, '2025-03-01'), (2, '2025-03-02'),\n",
    "    (2, '2025-03-03'), (2, '2025-03-04'), (3, '2025-03-01'),\n",
    "    (3, '2025-03-02'), (3, '2025-03-03'), (3, '2025-03-04'),\n",
    "    (3, '2025-03-04'), (3, '2025-03-04'), (3, '2025-03-05')\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 1. Create DataFrame and normalize types\n",
    "df = (spark.createDataFrame(data, ['user_id', 'login_date'])\n",
    "        .withColumn('login_date', to_date('login_date'))\n",
    "        .dropDuplicates(['user_id', 'login_date'])  # dedupe same-day logins\n",
    "     )\n",
    "df.display()\n",
    "# 2. Assign a row number per user ordered by date\n",
    "window = Window.partitionBy('user_id').orderBy('login_date')\n",
    "df = df.withColumn('rn', row_number().over(window))\n",
    "df.display()\n",
    "# 3. Compute a DATE-based â€œgroup keyâ€ by shifting each date back by its row number\n",
    "df = df.withColumn('grp', date_sub(col('login_date'), col('rn')))\n",
    "df.display()\n",
    "# 4. Aggregate per user+grp to get streaks\n",
    "streaks = (\n",
    "    df.groupBy('user_id', 'grp')\n",
    "      .agg(\n",
    "         count('*').alias('consecutive_days'),\n",
    "         min('login_date').alias('start_date'),\n",
    "         max('login_date').alias('end_date')\n",
    "      )\n",
    "      .filter(col('consecutive_days') >= 5)\n",
    "      .select('user_id', 'start_date', 'end_date', 'consecutive_days')\n",
    ")\n",
    "\n",
    "streaks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528439d7-e873-47c8-a158-5746ce3725cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b33fbd0-8742-40ed-be0d-5b153a451755",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"line\":393},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754494298365}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, lower, col\n",
    "\n",
    "# Sample input\n",
    "text_data = [\"Hello world\", \"Hello Spark\", \"Spark is fast and scalable\"]\n",
    "\n",
    "# Create a DataFrame from the list of strings\n",
    "df = spark.createDataFrame([(line,) for line in text_data], [\"line\"])\n",
    "df.display()\n",
    "df_temp = df.select(explode(split(lower(col(\"line\")), \" \")).alias(\"word\"))\n",
    "df_temp.display()\n",
    "# Split lines into words, explode the array to rows, normalize case, group & count\n",
    "word_counts = df.select(explode(split(lower(col(\"line\")), \" \")).alias(\"word\")) \\\n",
    "                .groupBy(\"word\") \\\n",
    "                .count()\n",
    "\n",
    "# Show the result\n",
    "word_counts.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7610093370668722,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "HandsOnPractice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
