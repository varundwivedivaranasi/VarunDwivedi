{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396570d7-f245-4eeb-ad4d-0f954958fb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample sales data\n",
    "data = [\n",
    "    (\"ProductA\", \"2024-01\", 100),\n",
    "    (\"ProductA\", \"2024-02\", 120),\n",
    "    (\"ProductA\", \"2024-03\", 900),\n",
    "    (\"ProductB\", \"2024-01\", 200),\n",
    "    (\"ProductB\", \"2024-02\", 210),\n",
    "    (\"ProductB\", \"2024-03\", 200),\n",
    "    (\"ProductC\", \"2024-03\", 250),\n",
    "]\n",
    "\n",
    "columns = [\"Product\", \"Month\", \"Revenue\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c6299a-f66d-407e-8d11-2a6b46fa0131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window partitioned by product and ordered by month\n",
    "windowSpec = Window.partitionBy(\"Product\").orderBy(\"Month\")\n",
    "\n",
    "# Add lag and lead columns\n",
    "df_with_lag_lead = df.withColumn(\"Prev_Revenue\", lag(\"Revenue\", 1).over(windowSpec)) \\\n",
    "                     .withColumn(\"Next_Revenue\", lead(\"Revenue\", 1).over(windowSpec))\n",
    "\n",
    "#df_with_lag_lead.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d123d4-e358-427c-b5c5-6e3ff2e83946",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753598166316}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        round((((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)),2).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn(\"Flag\",lit(\"Y\"))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce365d5d-6bff-43c6-aaaa-74bbc1897850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def log_errors_to_blob(error_messages: dict, notebook_path: str, container_path: str, file_prefix: str = \"schema_log\"):\n",
    "    \"\"\"\n",
    "    Logs error messages to a JSON file in Azure Blob Storage with notebook metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - error_messages: Dictionary of error keys with list of issues\n",
    "    - notebook_path: Full notebook path string\n",
    "    - container_path: Blob container path (e.g., \"/mnt/logs/schema/\")\n",
    "    - file_prefix: Optional prefix for filename\n",
    "    \"\"\"\n",
    "    #spark = SparkSession.builder.getOrCreate()\n",
    "    #dbutils = DBUtils(spark)\n",
    "\n",
    "    metadata = {\n",
    "        \"notebook_path\": notebook_path,\n",
    "        \"timestamp_utc\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"user\": dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get(),\n",
    "        \"cluster_id\": dbutils.notebook.entry_point.getDbutils().notebook().getContext().clusterId().get()\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"metadata\": metadata,\n",
    "        \"errors\": error_messages\n",
    "    }\n",
    "\n",
    "    file_name = f\"{file_prefix}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    full_path = f\"{container_path}/{file_name}\"\n",
    "\n",
    "    dbutils.fs.put(full_path, json.dumps(payload, indent=2), overwrite=True)\n",
    "    print(f\"Error log written to: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5573a3d5-a9af-4125-b577-ac9043d71d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "def validate_schema(df: DataFrame, table_name: str, strict: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Validates schema of DataFrame against columns of a Hive table.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Incoming DataFrame to validate\n",
    "    - table_name: Hive table name to compare against\n",
    "    - strict: If True, exact match required. If False, allows subset match\n",
    "\n",
    "    Returns:\n",
    "    - True if schema matches (based on strict mode), False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        expected_cols = [field.name for field in spark.table(table_name).schema.fields]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve schema for table '{table_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "    actual_cols = df.columns\n",
    "\n",
    "\n",
    "    \n",
    "    # Fetch schema from rro_sales_data\n",
    "    table_schema = spark.table(\"rro.sales_data\").schema\n",
    "    #display(table_schema)\n",
    "    # Extract column names\n",
    "    expected_cols1 = [field.name for field in table_schema]\n",
    "    actual_columns = set(df.columns)\n",
    "    expected_set = set(expected_cols1)  \n",
    "    missing = expected_set - actual_columns\n",
    "    extras = actual_columns - expected_set #if strict else set()\n",
    "    \n",
    "    if missing:\n",
    "        schema_error_dict = {\n",
    "        \"Missing Columns\": sorted(missing)\n",
    "        }\n",
    "\n",
    "        log_errors_to_blob(\n",
    "        error_messages=schema_error_dict,\n",
    "        notebook_path=dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get(),\n",
    "        container_path=\"abfss://gold@rmpyru.dfs.core.windows.net/errorlog\",\n",
    "        file_prefix=table_name\n",
    "    )\n",
    "    if extras:\n",
    "        schema_error_dict = {\n",
    "        \"Unexpected columns (strict mode)\": sorted(extras)\n",
    "        }\n",
    "        log_errors_to_blob(\n",
    "        error_messages=schema_error_dict,\n",
    "        notebook_path=dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get(),\n",
    "        container_path=\"abfss://gold@rmpyru.dfs.core.windows.net/errorlog\",\n",
    "        file_prefix=table_name\n",
    "    )\n",
    "    \n",
    "    if strict:\n",
    "        return set(actual_cols) == set(expected_cols)\n",
    "    else:\n",
    "        return set(expected_cols).issubset(set(actual_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d5264a-93c2-47ae-8272-03e7b7a12934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"TableName\", \"rro.sales_data\")\n",
    "table_name = dbutils.widgets.get(\"TableName\")\n",
    "# Assuming df_with_lag_lead_wri is your processed DataFrame\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "merge_condition = \"\"\"\n",
    "source.Product = target.Product AND\n",
    "source.Month = target.Month\n",
    "\"\"\"\n",
    "\n",
    "if validate_schema(df_with_lag_lead_wri, table_name, strict=False):\n",
    "    #df_with_lag_lead_wri.write.mode(\"append\").saveAsTable(table_name)\n",
    "    (\n",
    "    DeltaTable.forName(spark, table_name).alias(\"target\")\n",
    "    .merge(\n",
    "        df_with_lag_lead_wri.alias(\"source\"),\n",
    "        merge_condition\n",
    "        )\n",
    "    .whenMatchedUpdateAll()   # Updates all columns if match found\n",
    "    .whenNotMatchedInsertAll()  # Inserts new rows if no match\n",
    "    .execute()\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Schema mismatch detected. Investigate before writing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a4937e-68c5-48f0-b353-a8ff59f58020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json = 'abfss://gold@rmpyru.dfs.core.windows.net/errorlog'\n",
    "df_error = spark.read.format('json')\\\n",
    "                .option('inferSchema',True)\\\n",
    "                .option('multiLine',True)\\\n",
    "                .option('header',True)\\\n",
    "                .load(f'{json}/rro.sales_data_*.json')\n",
    "\n",
    "# Flatten into tabular format\n",
    "df_report = df_error.selectExpr(\n",
    "    \"metadata.notebook_path as NotebookPath\",\n",
    "    \"metadata.timestamp_utc as TimestampUTC\",\n",
    "    \"metadata.user as Username\",\n",
    "    \"metadata.cluster_id as ClusterID\",\n",
    "    \"errors as Errors\"\n",
    ")\n",
    "\n",
    "df_report.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91071d22-fd1a-4381-b2f8-f78e37a8151e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (select * from rro.sales_data order by Product,Month) SELECT `Product`,SUM(`Revenue`) `column_c5f738ab66`,`Product` FROM q GROUP BY `Product`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "Product",
             "id": "column_c5f738ab69"
            },
            "x": {
             "column": "Product",
             "id": "column_c5f738ab68"
            },
            "y": [
             {
              "column": "Revenue",
              "id": "column_c5f738ab66",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_c5f738ab66": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "b8be5b83-c0b4-4541-8594-c92d1b6dce43",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 9.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Product",
           "type": "column"
          },
          {
           "column": "Product",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Product",
           "type": "column"
          },
          {
           "alias": "column_c5f738ab66",
           "args": [
            {
             "column": "Revenue",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "Product",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--ALTER TABLE rro.sales_data SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name');ALTER TABLE rro.sales_data DROP COLUMN Flag\n",
    "--ALTER TABLE rro.sales_data DROP COLUMN Flag1\n",
    "--delete from rro.sales_data where 1=1\n",
    "select * from rro.sales_data order by Product,Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8465c30c-d8a8-4f7c-8fa3-71e49330e2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7525686479668195,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DynamicSchemaValidation",
   "widgets": {
    "TableName": {
     "currentValue": "rro.sales_data",
     "nuid": "264e2b61-e507-4e6e-93a1-fec06b0a692f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "rro.sales_data",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "rro.sales_data",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
