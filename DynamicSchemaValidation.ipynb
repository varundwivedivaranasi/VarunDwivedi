{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396570d7-f245-4eeb-ad4d-0f954958fb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, lead, lit, concat\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample sales data\n",
    "data = [\n",
    "    (\"ProductA\", \"2024-01\", 100),\n",
    "    (\"ProductA\", \"2024-02\", 120),\n",
    "    (\"ProductA\", \"2024-03\", 90),\n",
    "    (\"ProductB\", \"2024-01\", 200),\n",
    "    (\"ProductB\", \"2024-02\", 210),\n",
    "    (\"ProductB\", \"2024-03\", 200),\n",
    "]\n",
    "\n",
    "columns = [\"Product\", \"Month\", \"Revenue\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c6299a-f66d-407e-8d11-2a6b46fa0131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window partitioned by product and ordered by month\n",
    "windowSpec = Window.partitionBy(\"Product\").orderBy(\"Month\")\n",
    "\n",
    "# Add lag and lead columns\n",
    "df_with_lag_lead = df.withColumn(\"Prev_Revenue\", lag(\"Revenue\", 1).over(windowSpec)) \\\n",
    "                     .withColumn(\"Next_Revenue\", lead(\"Revenue\", 1).over(windowSpec))\n",
    "\n",
    "#df_with_lag_lead.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d123d4-e358-427c-b5c5-6e3ff2e83946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn('Flag', lit('Flag'))\n",
    "#df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5573a3d5-a9af-4125-b577-ac9043d71d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def validate_schema(df: DataFrame, table_name: str, strict: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Validates schema of DataFrame against columns of a Hive table.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Incoming DataFrame to validate\n",
    "    - table_name: Hive table name to compare against\n",
    "    - strict: If True, exact match required. If False, allows subset match\n",
    "\n",
    "    Returns:\n",
    "    - True if schema matches (based on strict mode), False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        expected_cols = [field.name for field in spark.table(table_name).schema.fields]\n",
    "    except Exception as e:\n",
    "        print(f\"[❌] Failed to retrieve schema for table '{table_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "    actual_cols = df.columns\n",
    "\n",
    "    if strict:\n",
    "        return set(actual_cols) == set(expected_cols)\n",
    "    else:\n",
    "        return set(expected_cols).issubset(set(actual_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d5264a-93c2-47ae-8272-03e7b7a12934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"TableName\", \"rro.sales_data\")\n",
    "table_name = dbutils.widgets.get(\"TableName\")\n",
    "# Assuming df_with_lag_lead_wri is your processed DataFrame\n",
    "if validate_schema(df_with_lag_lead_wri, table_name, strict=True):\n",
    "    df_with_lag_lead_wri.write.mode(\"append\").saveAsTable(table_name)\n",
    "else:\n",
    "    print(\"[⚠️] Schema mismatch detected. Investigate before writing!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DynamicSchemaValidation",
   "widgets": {
    "TableName": {
     "currentValue": "rro.sales_data",
     "nuid": "264e2b61-e507-4e6e-93a1-fec06b0a692f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "rro.sales_data",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "rro.sales_data",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
