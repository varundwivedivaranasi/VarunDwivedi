{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396570d7-f245-4eeb-ad4d-0f954958fb26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample sales data\n",
    "data = [\n",
    "    (\"ProductA\", \"2024-01\", 100),\n",
    "    (\"ProductA\", \"2024-02\", 120),\n",
    "    (\"ProductA\", \"2024-03\", 900),\n",
    "    (\"ProductB\", \"2024-01\", 200),\n",
    "    (\"ProductB\", \"2024-02\", 210),\n",
    "    (\"ProductB\", \"2024-03\", 200),\n",
    "    (\"ProductC\", \"2024-03\", 250),\n",
    "]\n",
    "\n",
    "columns = [\"Product\", \"Month\", \"Revenue\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c6299a-f66d-407e-8d11-2a6b46fa0131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define window partitioned by product and ordered by month\n",
    "windowSpec = Window.partitionBy(\"Product\").orderBy(\"Month\")\n",
    "\n",
    "# Add lag and lead columns\n",
    "df_with_lag_lead = df.withColumn(\"Prev_Revenue\", lag(\"Revenue\", 1).over(windowSpec)) \\\n",
    "                     .withColumn(\"Next_Revenue\", lead(\"Revenue\", 1).over(windowSpec))\n",
    "\n",
    "#df_with_lag_lead.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d123d4-e358-427c-b5c5-6e3ff2e83946",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753598166316}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_lag_lead_wri = df_with_lag_lead.withColumn(\n",
    "    \"Revenue_Gap\", \n",
    "    concat(\n",
    "        (((col(\"Revenue\") - col(\"Prev_Revenue\")) / col(\"Revenue\")) * lit(100)).cast(\"string\"), \n",
    "        lit('%')\n",
    "    )\n",
    ").withColumn(\"Flag\",lit(\"Y\"))\n",
    "df_with_lag_lead_wri.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce365d5d-6bff-43c6-aaaa-74bbc1897850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def log_errors_to_blob(error_messages: dict, notebook_path: str, container_path: str, file_prefix: str = \"schema_log\"):\n",
    "    \"\"\"\n",
    "    Logs error messages to a JSON file in Azure Blob Storage with notebook metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - error_messages: Dictionary of error keys with list of issues\n",
    "    - notebook_path: Full notebook path string\n",
    "    - container_path: Blob container path (e.g., \"/mnt/logs/schema/\")\n",
    "    - file_prefix: Optional prefix for filename\n",
    "    \"\"\"\n",
    "    #spark = SparkSession.builder.getOrCreate()\n",
    "    #dbutils = DBUtils(spark)\n",
    "\n",
    "    metadata = {\n",
    "        \"notebook_path\": notebook_path,\n",
    "        \"timestamp_utc\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"user\": dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get(),\n",
    "        \"cluster_id\": dbutils.notebook.entry_point.getDbutils().notebook().getContext().clusterId().get()\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"metadata\": metadata,\n",
    "        \"errors\": error_messages\n",
    "    }\n",
    "\n",
    "    file_name = f\"{file_prefix}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    full_path = f\"{container_path}/{file_name}\"\n",
    "\n",
    "    dbutils.fs.put(full_path, json.dumps(payload, indent=2), overwrite=True)\n",
    "    print(f\"Error log written to: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5573a3d5-a9af-4125-b577-ac9043d71d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "def validate_schema(df: DataFrame, table_name: str, strict: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Validates schema of DataFrame against columns of a Hive table.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Incoming DataFrame to validate\n",
    "    - table_name: Hive table name to compare against\n",
    "    - strict: If True, exact match required. If False, allows subset match\n",
    "\n",
    "    Returns:\n",
    "    - True if schema matches (based on strict mode), False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        expected_cols = [field.name for field in spark.table(table_name).schema.fields]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve schema for table '{table_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "    actual_cols = df.columns\n",
    "\n",
    "\n",
    "    \n",
    "    # Fetch schema from rro_sales_data\n",
    "    table_schema = spark.table(\"rro.sales_data\").schema\n",
    "    #display(table_schema)\n",
    "    # Extract column names\n",
    "    expected_cols1 = [field.name for field in table_schema]\n",
    "    actual_columns = set(df.columns)\n",
    "    expected_set = set(expected_cols1)  \n",
    "    missing = expected_set - actual_columns\n",
    "    extras = actual_columns - expected_set #if strict else set()\n",
    "    \n",
    "    if missing:\n",
    "        schema_error_dict = {\n",
    "        \"Missing Columns\": sorted(missing)\n",
    "        }\n",
    "\n",
    "        log_errors_to_blob(\n",
    "        error_messages=schema_error_dict,\n",
    "        notebook_path=dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get(),\n",
    "        container_path=\"abfss://gold@rmpyru.dfs.core.windows.net/errorlog\",\n",
    "        file_prefix=table_name\n",
    "    )\n",
    "    if extras:\n",
    "        schema_error_dict = {\n",
    "        \"Unexpected columns (strict mode)\": sorted(extras)\n",
    "        }\n",
    "        log_errors_to_blob(\n",
    "        error_messages=schema_error_dict,\n",
    "        notebook_path=dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get(),\n",
    "        container_path=\"abfss://gold@rmpyru.dfs.core.windows.net/errorlog\",\n",
    "        file_prefix=table_name\n",
    "    )\n",
    "    \n",
    "    if strict:\n",
    "        return set(actual_cols) == set(expected_cols)\n",
    "    else:\n",
    "        return set(expected_cols).issubset(set(actual_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1d5264a-93c2-47ae-8272-03e7b7a12934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"TableName\", \"rro.sales_data\")\n",
    "table_name = dbutils.widgets.get(\"TableName\")\n",
    "# Assuming df_with_lag_lead_wri is your processed DataFrame\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "merge_condition = \"\"\"\n",
    "source.Product = target.Product AND\n",
    "source.Month = target.Month\n",
    "\"\"\"\n",
    "\n",
    "if validate_schema(df_with_lag_lead_wri, table_name, strict=False):\n",
    "    #df_with_lag_lead_wri.write.mode(\"append\").saveAsTable(table_name)\n",
    "    (\n",
    "    DeltaTable.forName(spark, table_name).alias(\"target\")\n",
    "    .merge(\n",
    "        df_with_lag_lead_wri.alias(\"source\"),\n",
    "        merge_condition\n",
    "        )\n",
    "    .whenMatchedUpdateAll()   # Updates all columns if match found\n",
    "    .whenNotMatchedInsertAll()  # Inserts new rows if no match\n",
    "    .execute()\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"Schema mismatch detected. Investigate before writing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a4937e-68c5-48f0-b353-a8ff59f58020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json = 'abfss://gold@rmpyru.dfs.core.windows.net/errorlog'\n",
    "df_error = spark.read.format('json')\\\n",
    "                .option('inferSchema',True)\\\n",
    "                .option('multiLine',True)\\\n",
    "                .option('header',True)\\\n",
    "                .load(f'{json}/rro.sales_data_*.json')\n",
    "\n",
    "# Flatten into tabular format\n",
    "df_report = df_error.selectExpr(\n",
    "    \"metadata.notebook_path as NotebookPath\",\n",
    "    \"metadata.timestamp_utc as TimestampUTC\",\n",
    "    \"metadata.user as Username\",\n",
    "    \"metadata.cluster_id as ClusterID\",\n",
    "    \"errors as Errors\"\n",
    ")\n",
    "\n",
    "df_report.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91071d22-fd1a-4381-b2f8-f78e37a8151e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--ALTER TABLE rro.sales_data SET TBLPROPERTIES ('delta.columnMapping.mode' = 'name');ALTER TABLE rro.sales_data DROP COLUMN Flag\n",
    "--ALTER TABLE rro.sales_data DROP COLUMN Flag1\n",
    "--delete from rro.sales_data where 1=1\n",
    "select * from rro.sales_data order by Product,Month"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6638283158354900,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DynamicSchemaValidation",
   "widgets": {
    "TableName": {
     "currentValue": "rro.sales_data",
     "nuid": "264e2b61-e507-4e6e-93a1-fec06b0a692f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "rro.sales_data",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "rro.sales_data",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
