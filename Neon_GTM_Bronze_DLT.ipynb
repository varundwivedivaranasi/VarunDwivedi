{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799d72e5-9007-4ae8-84c8-3b388ea17815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG neon_gtm;\n",
    "USE SCHEMA bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f10b5206-01d1-4287-bf38-fa3dabae25d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7238ac3d-c152-4f7e-9f9d-0d98ccbdaa68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expr, lit, to_timestamp, create_map\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Load metadata from control tables\n",
    "# ----------------------------------------\n",
    "\n",
    "source_name = \"telecomx_sensor_csv\"\n",
    "\n",
    "config = spark.table(\"neon_gtm.bronze.data_source_config\") \\\n",
    "    .filter(F.col(\"source_name\") == source_name) \\\n",
    "    .first()\n",
    "\n",
    "schema_df = spark.table(\"neon_gtm.bronze.source_schema_definition\") \\\n",
    "    .filter(F.col(\"source_config_id\") == config.source_config_id)\n",
    "\n",
    "mapping_df = spark.table(\"neon_gtm.bronze.source_column_mapping\") \\\n",
    "    .filter(F.col(\"schema_id\").isin([r.schema_id for r in schema_df.collect()]))\n",
    "\n",
    "rules_df = spark.table(\"neon_gtm.bronze.data_quality_rule\") \\\n",
    "    .filter(F.col(\"schema_id\").isin([r.schema_id for r in schema_df.collect()]))\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Build schema dynamically\n",
    "# ----------------------------------------\n",
    "\n",
    "type_map = {\n",
    "    \"STRING\": StringType(),\n",
    "    \"FLOAT\": FloatType(),\n",
    "    \"TIMESTAMP\": TimestampType()\n",
    "}\n",
    "\n",
    "# Sort schema_df by ordinal_position before building schema\n",
    "ordered_schema_df = schema_df.orderBy(\"ordinal_position\")\n",
    "\n",
    "fields = [\n",
    "    StructField(row.column_name, type_map.get(row.data_type.upper(), StringType()), row.nullable)\n",
    "    for row in ordered_schema_df.collect()\n",
    "]\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "mapping_df = (\n",
    "    spark.table(\"neon_gtm.bronze.source_column_mapping\")\n",
    "    .alias(\"map\")\n",
    "    .join(\n",
    "        spark.table(\"neon_gtm.bronze.source_schema_definition\").alias(\"schema\"),\n",
    "        F.col(\"map.schema_id\") == F.col(\"schema.schema_id\")\n",
    "    )\n",
    "    .filter(F.col(\"schema.source_config_id\") == config.source_config_id)\n",
    ")\n",
    "# fetch targtet table name\n",
    "target_table = (\n",
    "    mapping_df.select(\"target_table_name\")\n",
    "    .distinct()\n",
    "    .first()[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2220eb4f-70f2-493b-9fd9-5af0f2713930",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"device_id\":212,\"power\":288,\"timestamp\":255},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757260376623}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3. Read source data using Auto Loader\n",
    "# ----------------------------------------\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .format(config.source_type)\n",
    "    .option(\"header\", config.connection_details[\"header\"])\n",
    "    .schema(schema)\n",
    "    .load(config.connection_details[\"path\"])\n",
    ")\n",
    "df_raw.display()\n",
    "# ----------------------------------------\n",
    "# 4. Apply transformation hints\n",
    "# ----------------------------------------\n",
    "\n",
    "for row in schema_df.collect():\n",
    "    if row.transformation_hint:\n",
    "        df_raw = df_raw.withColumn(row.column_name, expr(row.transformation_hint))\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5. Apply column mappings\n",
    "# ----------------------------------------\n",
    "\n",
    "for row in mapping_df.collect():\n",
    "    df_raw = df_raw.withColumnRenamed(row.source_column_name, row.target_column_name)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 6. Apply validation rules\n",
    "# ----------------------------------------\n",
    "\n",
    "for rule in rules_df.collect():\n",
    "    df_raw = df_raw.filter(rule.validation_expression)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 7. Enrich with static or metadata-driven values\n",
    "# ----------------------------------------\n",
    "\n",
    "# Load sensor metadata\n",
    "sensor_df = spark.table(\"neon_gtm.bronze.sensor\").select(\"equipment_id\", \"sensor_id\", \"unit_id\")\n",
    "\n",
    "# Load unit metadata\n",
    "unit_df = spark.table(\"neon_gtm.bronze.measurement_unit\").select(\"unit_id\", \"unit_type\")\n",
    "\n",
    "# Join sensor metadata\n",
    "df_enriched = df_raw \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\")) \\\n",
    "    .withColumnRenamed(\"device_id\", \"equipment_id\") \\\n",
    "    .join(sensor_df, on=\"equipment_id\", how=\"left\") \\\n",
    "    .join(unit_df, on=\"unit_id\", how=\"left\")\n",
    "\n",
    "# Final transformation\n",
    "df_transformed = df_enriched \\\n",
    "    .withColumnRenamed(\"power\", \"metric_value\") \\\n",
    "    .withColumn(\"metric_name\", F.col(\"unit_type\")) \\\n",
    "    .withColumn(\"reading_id\", F.expr(\"uuid()\")) \\\n",
    "    .withColumn(\"metadata\", create_map(F.lit(\"source\"), F.lit(source_name))) \\\n",
    "    .drop(\"unit_type\")\n",
    "# ----------------------------------------\n",
    "# 8. Write to bronze.sensor_reading\n",
    "# ----------------------------------------\n",
    "target_schema = spark.table(f\"neon_gtm.bronze.{target_table}\").schema\n",
    "not_null_cols = [f.name for f in target_schema.fields if not f.nullable]\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Build composite condition: all NOT NULL columns must be non-null\n",
    "valid_condition = expr(\" AND \".join([f\"{c} IS NOT NULL\" for c in not_null_cols]))\n",
    "\n",
    "# Apply quality_flag logic\n",
    "df_validated = df_transformed.withColumn(\n",
    "    \"quality_flag\",\n",
    "    when(valid_condition, lit(\"Valid\")).otherwise(lit(\"Invalid\"))\n",
    ")\n",
    "\n",
    "df_validated.filter(\"quality_flag = 'Invalid'\").display()\n",
    "df_validated.filter(\"quality_flag = 'Valid'\").write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"neon_gtm.bronze.{target_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6025235032056032,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Neon_GTM_Bronze_DLT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
